---
title: "ShovanBiswas-DATA24_Homework_09"
author: "Shovan Biswas"
date: "11/21/2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    highlight: pygments
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_chunk$set(fig.width = 8, fig.height = 8) 
```

# Libraries  

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(corrplot)
library(reshape2)
library(Amelia)
library(dlookr)
library(fpp2)
library(plotly)
library(gridExtra)
library(readxl)
library(ggplot2)
library(urca)
library(tseries)
library(AppliedPredictiveModeling)
library(RANN)
library(psych)
library(e1071)
library(corrplot)
library(glmnet)
library(mlbench)
library(caret)
library(earth)
library(randomForest)
library(party)
library(Cubist)
library(gbm)
library(rpart)
```

# Exercise 8.1  

Recreate the simulated data from Exercise 7.2:     

(This exercise is based on library(mlbench), which I included in libraries at the top.)     
```{r}
set.seed(200)

simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

(a) Fit a random forest model to all of the predictors, then estimate the variable importance scores:

(This exercise is based on library(randomForest) and library(caret), which I included in libraries at the top.)     
```{r}
model1 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
```

Did the random forest model significantly use the uninformative predictors (V6 â€“ V10)?          

```{r}
rfImp1
```

Almost 0 to negative values evince that Random Forest didn't use the predictors (V6 thru V10).         


(b) Now add an additional predictor that is highly correlated with one of the informative predictors. For example:        

```{r}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
```

Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?       

```{r}
model_new <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp_new <- varImp(model_new, scale = FALSE)

rfImp_new
```

V1's score indeed changed. While the score of V1 reduced from 8.732235404 to 5.69119973. importance of highly correlated duplicate1 is the least.          

Here's the answer to the last question. quoted from text book:        

"An advantage of tree-based models is that, when the tree is not large, the model is simple and interpretable. Also, this type of tree can be computed quickly (despite using multiple exhaustive searches). Tree models intrinsically conduct feature selection; if a predictor is never used in a split, the prediction equation is independent of these data. **This advantage is weakened when there are highly correlated predictors. If two predictors are extremely correlated, the choice of which to use in a split is somewhat random.**"        


(c) Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?         

(Requires library(party), which I included in libraries at the top.)          
```{r}
cforestModel <- cforest(y ~ ., data = simulated)
sort(varimp(cforestModel, conditional = FALSE), decreasing = TRUE)
```

```{r}
sort(varimp(cforestModel, conditional = TRUE), decreasing = TRUE)
```

By setting conditional = TRUE, it took longer to process and the results were different. The uninformative predictors (V6 thru V10) remain the same. By setting conditional = TRUE, V3 has become become uninformative.          


(d) Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?          

(Requires library(gbm), which I included in libraries at the top.)          
```{r}
gbmModel <- gbm(y ~ ., data = simulated, distribution = 'gaussian')   # refer page 216, under "Boosted Trees"
summary(gbmModel)
```

For Boosted Trees, I used gbm(). Same patten occurs. V4 is still the highest and V6 thru V10 are still low.        

(Requires library(Cubist), which I included in libraries at the top.)          
```{r}
cubistModel <- cubist(x = simulated[, -(ncol(simulated) - 1)], y = simulated$y)   # refer page 217, under "Cubist"
varImp(cubistModel)
```

For Cubist, I used cubist(). Same patten occurs. Here also V6 thru V10 remains the lowest in importance. Only duplicate1 has moved up by one notche.          


# Exercise 8.2        

Use a simulation to show tree bias with different granularities.          

We'll use rpart package, which is One of the widely used implementations for single regression trees in R. The function rpart splits based on CART methodology.          

First let's create some samples and collect them into variables x1, x2, x3, x4. Then we'll create a dataframe of 150 observations of x1 thru x4 and the response variable y.           

(Requires library(rpart), which I included in libraries at the top.)          
```{r}
set.seed(123)
x1 <- sample(0:10000 / 10000, 150)                   # 150 possible values in x1.
x2 <- sample(0:1000 / 1000, 150)                     # 150 possible values in x2.
x3 <- sample(0:100 / 100, 150, replace = TRUE)       # 100 possible values in x3, but with replacement to fill up 150 positions.
x4 <- sample(0:10 / 10, 150, replace = TRUE)         # 10  possible values in x4, but with replacement to fill up 150 positions. 

y <- x1 + x2

tree_bias <- data.frame(x1, x2, x3, x4, y)

head(tree_bias)
```

Now, we'll run the run rpart and find the variables of importance.         

Note: I tried function **ctree()** of the same rpart package rpart. It worked, but the function varImp() failed on its output.         
```{r}
rpartTree <- rpart(y ~ ., data = tree_bias)
varImp(rpartTree)
```

The order x1, x2, x3, x4 expresses the degree of granularity. Variable x1 is most split and x4 is least split.        

# Exercise 8.3        

In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:        

(a) Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?        

Bagging fraction of 0.1 means only 10% of the full data is randomly sampled. So, each tree may be built with different datasets altogether. Implies more variance. Therefore, the trees split very differently from each other. If the bagging is 0.9, in each run same dataset or almost same dataset is exposed to the trees. So, less variance. So, lesser splits.      

(Please refer page 207, 3rd para of textbook.)      

(b) Which model do you think would be more predictive of other samples?         

Based on the variance argument in (a) above, I would think that a model with a learning rate of 0.1 is more predictive.      

(c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24.        

As interaction depth is increased, the trees grow more deep. This causes more predictors to split. Therefore, vairable importance gets distributed over more variables, rather than less.         


# Exercise 8.7          

8.7. Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:           

## Below portion was brought forward from 7.5:      

(Requires library(AppliedPredictiveModeling), which I included in libraries at the top.)          

Reading data:         
```{r}
data(ChemicalManufacturingProcess)
dim(ChemicalManufacturingProcess)
```

Imputation:      
```{r}
data_imputed <- mice(ChemicalManufacturingProcess, m = 1, method = "pmm", print = F) %>% complete()
```

Checking the data:      
```{r}
any(is.na(data_imputed))
```

At this point, the data is imputed. I'll proceed to Box-Cox it.     

```{r}
# Initially identifying columns, whose skewness are not less than 1.
transform_cols <- c()

for(i in seq(from = 1, to = length(data_imputed), by = 1)) {
  if(abs(skewness(data_imputed[, i])) >= 1) {
    transform_cols <- append(transform_cols, names(data_imputed[i]))
  }
}

# Applying Box-cox.
lambda <- NULL
data_imputed_2 <- data_imputed

for (i in 1:length(transform_cols)) {
  lambda[transform_cols[i]] <- BoxCox.lambda(abs(data_imputed[, transform_cols[i]]))

  data_imputed_2[c(transform_cols[i])] <- BoxCox(data_imputed[transform_cols[i]], lambda[transform_cols[i]])
}
```

At this point, the data is pre-processed. The pre-processed data is stored in the variable data_imputed_2.     

So, I'll proceed to split the data into train and test in 80/20 ratio.      

```{r}
set.seed(200)

split_index <- createDataPartition(data_imputed_2$Yield, p = 0.8, list = FALSE)
X_train <- data_imputed_2[split_index, ]
y_train <- data_imputed_2$Yield[split_index]

X_test <- data_imputed_2[-split_index, ]
y_test <- data_imputed_2$Yield[-split_index]
```

At this point, the data is split and we can proceed to create 4 models gbm, rpart, cubist, rf.        

```{r, eval = TRUE, message = FALSE, warning = FALSE}
set.seed(200)

grid <- expand.grid(n.trees = c(50, 75, 100, 150), interaction.depth = c(1, 5, 10, 12), shrinkage = c(0.01, 0.1, 0.5), n.minobsinnode = c(5, 10, 12))

gbmModel <- train(x = X_train, y = y_train, method = 'gbm', tuneGrid = grid, verbose = F)

gbmModel
```


```{r, eval = TRUE, message = FALSE, warning = FALSE}
set.seed(200)

rpartModel <- train(x = X_train, y = y_train, method = 'rpart', tuneLength = 10, control = rpart.control(maxdepth = 2)) # formula from page 239, chapter 10 of textbook.

rpartModel
```

For some strange reason, all the RMSE values in rpart are coming to be the same. This is affecting the downstream analysis.          

```{r, eval = TRUE, message = FALSE, warning = FALSE}
set.seed(200)

cubistModel <- train(x = X_train, y = y_train, method = 'cubist')

cubistModel
```

**For some strange reason, all the RMSE values in cubist are coming to be the same. This is affecting the downstream analysis.**          

```{r, eval = TRUE, message = FALSE, warning = FALSE}
set.seed(200)

rfModel <- train(x = X_train, y = y_train, method = 'rf', tuneLength = 10)

rfModel
```

At this point all the models were run, and we are in a position to run the 

(a) Which tree-based regression model gives the optimal resampling and test set performance?       

Let's first assemble all the models.        

```{r}
samp <- resamples(list(GradientBoosting = gbmModel, SingleTree = rpartModel, Cubist = cubistModel, RandomForest = rfModel))

summary(samp)
```

Now performance testing, we already have the models. All we have to do is predict on test and compare the RMSE.         

```{r}
perf_testing <- function(models, testData, testTarget) {
  method <- c()
  df <- data.frame()
  for(model in models){
    method <- c(method, model$method)
    pred <- predict(model, newdata = testData)
    df <- rbind(df, t(postResample(pred = pred, obs = testTarget)))
  }
  row.names(df) <- method
  return(df)
}

models <- list(gbmModel, rpartModel, cubistModel, rfModel)

performance <- perf_testing(models, X_test, y_test)

performance[order(performance$RMSE),]
```

In order of performance, cubit is the best, followed by gbm, rf and rpart.       


(b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?           

We found Cubist to be the optimal. So, we'll check the varImp() of cubistModel.      
```{r}
varImp(cubistModel)
```

We observed above that the RMSE values in cubist were same. That was very weird. It affected the importance of the predictors. Only ManufacturingProcess02 has an Overall of 100. The rest are all 0.                


(c) Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?          

The plot is as follows:       

```{r}
plot(rpartModel$finalModel)
text(rpartModel$finalModel)
```

